spring.datasource.url=jdbc:mysql://10.100.62.5:3306/data_manager?characterEncoding=UTF-8&serverTimezone=Asia/Shanghai
#spring.datasource.url=jdbc:mysql://10.20.10.173:3306/data_manager?characterEncoding=UTF-8&serverTimezone=Asia/Shanghai
spring.datasource.type=com.alibaba.druid.pool.DruidDataSource
spring.datasource.driver-class-name=com.mysql.jdbc.Driver
spring.datasource.username=root
spring.datasource.password=cetc38
#spring.datasource.password=root


# cloudera name node url
cloudera.hdfs.namenode.url=http://10.100.62.5:9870/jmx?qry=Hadoop:service=NameNode,name=FSNamesystemState
#cloudera.hdfs.namenode.url=http://10.20.10.171:9870/jmx?qry=Hadoop:service=NameNode,name=FSNamesystemState

# user data directory path, self configuration, please make sure the directory exists and have read write permissions
#data.basedir.path=D:///tmp
data.basedir.path=/tmp

# directory path for user data download. self configuration, please make sure the directory exists and have read write permissions
#data.download.basedir.path=D:///tmp/download
data.download.basedir.path=/tmp/download

# directory path for user data upload. self configuration, please make sure the directory exists and have read write permissions
#data.upload.basedir.path=D:///tmp/tmp/upload
data.upload.basedir.path=/tmp/tmp/upload

# ha or single namenode,If namenode ha needs to copy core-site.xml and hdfs-site.xml to the conf directory
fs.defaultFS=hdfs://10.100.62.5:8020
#fs.defaultFS=hdfs://10.20.10.171:8020

cluster.ip=10.100.62.5
cluster.user.name=admin
cluster.user.password=admin

#http.defaultURL=http://10.20.10.171:9870/webhdfs/v1
http.defaultURL=http://10.100.62.5:9870/webhdfs/v1

# host
#bd1.bcht=10.20.10.171
#bd2.bcht=10.20.10.172
#bd3.bcht=10.20.10.173
data5-nn1=10.100.62.5
data4-nn2=10.100.62.4
data3-dn1=10.100.62.3
data2-dn2=10.100.62.2
data1-dn3=10.100.62.1
